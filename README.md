# üöÄ Awesome Long2Short Papers

A curated list of papers about making LLM reasoning more efficient (shorter/faster/better).

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arXiv](https://img.shields.io/badge/arXiv-Papers-b31b1b.svg)](https://arxiv.org)
[![Update](https://img.shields.io/badge/Update-Daily-green.svg)](https://github.com/your-username/awesome-long2short-papers)

## üì∞ News

üéâ **Latest Update**: Our paper collection is continuously updated. Feel free to contribute!

## üìù Citation

If you find this resource helpful, please cite the relevant papers:

```bibtex
@misc{awesome-long2short-papers,
  title={Awesome Long2Short Papers},
  author={Community Contributors},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/yzhangchuck/awesome-llm-reasoning-long2short-papers}}
}
```

## üìö Table of Contents

- [Analysis and Understanding](#-analysis-and-understanding)
- [Inference Intervention](#-inference-intervention)
- [Latent Space Reasoning](#-latent-space-reasoning)
- [Supervised Fine-tuning](#-supervised-fine-tuning)
- [Steering Vector](#Ô∏è-steering-vector)
- [Reinforcement Learning Approaches](#-reinforcement-learning-approaches)
- [General Papers](#-general-papers)


---


## üîç Analysis and Understanding

Deep dives into LLM reasoning

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs** | 2024 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2412.21187) | - |
| **How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2503.01141) | - |
| **Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.18080) | - |


## ‚ö° Inference Intervention

Interventions during model inference process

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **When More is Less- Understanding Chain-of-Thought Length in LLMs** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.07266) | - |


## üß† Latent Space Reasoning

Reasoning in latent spaces

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **Compressed Chain of Thought- Efficient Reasoning through Dense Representations** | 2024 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2412.13171) | - |
| **LightThinker- Thinking Step-by-Step Compression** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.15589) | - |
| **Quiet-STaR- Language Models Can Teach Themselves to Think Before Speaking** | 2024 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2403.09629) | - |
| **Scaling up Test-Time Compute with Latent Reasoning- A Recurrent Depth Approach** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.05171) | - |
| **Token Assorted- Mixing Latent and Text Tokens for Improved Language Model Reasoning** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.03275) | - |
| **Training Large Language Models to Reason in a Continuous Latent Space** | 2024 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2412.06769) | - |


## üìö Supervised Fine-tuning

Direct optimization for efficiency

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **Self-Training Elicits Concise Reasoning in Large Language Models** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.20122) | - |
| **TokenSkip- Controllable Chain-of-Thought Compression in LLMs** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.12067) | - |
| **s1- Simple test-time scaling** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2501.19393) | - |


## üéõÔ∏è Steering Vector

Steering model behavior through vector manipulation

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **CoT-Valve- Length-Compressible Chain-of-Thought Tuning** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2501.19393) | - |



## üéØ Reinforcement Learning Approaches

Training models to reason efficiently

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **KIMI K1.5- SCALING REINFORCEMENT LEARNING WITH LLMS** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2501.12599) | - |
| **L1- Controlling How Long A Reasoning Model Thinks With Reinforcement Learning** | 2025 | arXiv preprint | [[Paper]](https://www.arxiv.org/abs/2503.04697) | - |
| **O1-Pruner- Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning** | 2024 | arXiv preprint | [[Paper]](https://www.arxiv.org/abs/2503.04697) | - |
| **Training Language Models to Reason Efficiently** | 2024 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.04463) | - |


## üåü General Papers

Latest advances in efficient reasoning

| Title | Year | Venue | Paper | Code |
|-------|------|-------|-------|------|
| **S*: Test Time Scaling for Code Generation** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.14382) | - |
| **Efficiently Serving LLM Reasoning Programs with Certaindex** | 2024 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2412.20993) | - |
| **Inner Thinking Transformer- Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.13842) | - |
| **Learning to Reason from Feedback at Test-Time** | 2025 | arXiv preprint | [[Paper]](https://arxiv.org/abs/2502.15771) | - |


## ü§ù Contributing

We welcome contributions! Please feel free to submit a pull request to add more papers or improve the existing content.

### Contribution Guidelines

1. Please ensure the paper is related to efficient LLM reasoning
2. Follow the existing format
3. Add a brief description if possible
4. Include links to paper and code (if available)


## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ‚≠ê Star History

[![Star History Chart](https://api.star-history.com/svg?repos=your-username/awesome-long2short-papers&type=Date)](https://star-history.com/#your-username/awesome-long2short-papers&Date)
